# Crawler---PHP-Codeigniter
A Web crawler, sometimes called a spider, is an Internet bot that systematically browses the World Wide Web, 
typically for the purpose of Web indexing (web spidering).  Web search engines and some other sites use 
Web crawling or spidering software to update their web content or indices of others sites' web content. 
Web crawlers can copy all the pages they visit for later processing by a search engine which indexes the downloaded pages 
so the users can search much more efficiently.  Crawlers consume resources on the systems they visit and often visit sites 
without approval. Issues of schedule, load, and "politeness" come into play when large collections of pages are accessed. 
Mechanisms exist for public sites not wishing to be crawled to make this known to the crawling agent. For instance, including a robots.txt 
file can request bots to index only parts of a website, or nothing at all.  As the number of pages on the internet is extremely large, 
even the largest crawlers fall short of making a complete index. For that reason search engines were bad at giving relevant search results 
in the early years of the World Wide Web, before the year 2000. This is improved greatly by modern search engines; nowadays very good 
results are given instantly.


A Web crawler starts with a list of URLs to visit, called the seeds. As the crawler visits these URLs, it identifies all the hyperlinks in the page and adds them to the list of URLs to visit, called the crawl frontier. URLs from the frontier are recursively visited according to a set of policies. If the crawler is performing archiving of websites it copies and saves the information as it goes. The archives are usually stored in such a way they can be viewed, read and navigated as they were on the live web, but are preserved as â€˜snapshots'.[4]

The archive is known as the repository and is designed to store and manage the collection of web pages. The repository only stores HTML pages and these pages are stored as distinct files. A repository is similar to any other system that stores data, like a modern day database. The only difference is that a repository does not need all the functionality offered by a database system. The repository stores the most recent version of the web page retrieved by the crawler.[5]

The large volume implies the crawler can only download a limited number of the Web pages within a given time, so it needs to prioritize its downloads. The high rate of change can imply the pages might have already been updated or even deleted.

The number of possible URLs crawled being generated by server-side software has also made it difficult for web crawlers to avoid retrieving duplicate content. Endless combinations of HTTP GET (URL-based) parameters exist, of which only a small selection will actually return unique content. For example, a simple online photo gallery may offer three options to users, as specified through HTTP GET parameters in the URL. If there exist four ways to sort images, three choices of thumbnail size, two file formats, and an option to disable user-provided content, then the same set of content can be accessed with 48 different URLs, all of which may be linked on the site. This mathematical combination creates a problem for crawlers, as they must sort through endless combinations of relatively minor scripted changes in order to retrieve unique content.

As Edwards et al. noted, "Given that the bandwidth for conducting crawls is neither infinite nor free, it is becoming essential to crawl the Web in not only a scalable, but efficient way, if some reasonable measure of quality or freshness is to be maintained."[6] A crawler must carefully choose at each step which pages to visit next.
